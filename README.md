# NewsRaag Crawler

**Advanced AI-Powered Financial News Aggregation & Vector Indexing System**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Azure](https://img.shields.io/badge/Azure-OpenAI%20%7C%20Blob%20%7C%20Insights-blue.svg)](https://azure.microsoft.com)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector%20DB-green.svg)](https://qdrant.tech)
[![Tests](https://img.shields.io/badge/Tests-Pytest-orange.svg)](https://pytest.org)

A production-ready financial news crawler with AI-powered content cleaning, semantic vector indexing, and enterprise-grade monitoring. Processes multiple financial news sources through RSS and web scraping, cleans content with Azure OpenAI GPT-4, and stores in vector databases for semantic search.

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone <your-repo-url>
cd newsraag-crawler

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers (for web scraping)
playwright install chromium --with-deps

# Configure environment
cp .env.example .env
# Edit .env with your Azure OpenAI, Qdrant, and storage credentials

# Test source configuration
python main.py --test-sources

# Run crawler
python main.py
```

## ğŸ“Š Architecture Overview

### **Enhanced Unified Source System**
The crawler uses a sophisticated factory pattern with multiple processing templates:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Source Factory                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RSS Sources        â”‚  HTML Scraping     â”‚  Hybrid Sources â”‚
â”‚  â€¢ BabyPips         â”‚  â€¢ Kabutan         â”‚  â€¢ MarketWatch  â”‚
â”‚  â€¢ FXStreet         â”‚  â€¢ Custom Sites    â”‚  â€¢ Future       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Hierarchical Processing Pipeline              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Crawl4AI (Primary)  â†’  2. BeautifulSoup (Fallback)     â”‚
â”‚  3. RSS Parser (Final)  â†’  4. Content Extraction           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AI Content Processing                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Azure OpenAI GPT-4 Content Cleaning                     â”‚
â”‚  â€¢ Financial Data Preservation                             â”‚
â”‚  â€¢ Navigation/Ad Removal                                    â”‚
â”‚  â€¢ Text Embedding Generation (3072-dim)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Storage & Indexing                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Qdrant Vector Database (Semantic Search)                â”‚
â”‚  â€¢ Azure Blob Storage (Document Archive)                   â”‚
â”‚  â€¢ Redis Cache (Optional Performance Layer)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Core Features

### **Intelligent Multi-Source Processing**
- âœ… **Unified Source Factory**: Supports RSS, HTML scraping, and hybrid sources
- âœ… **Hierarchical Extraction**: Crawl4AI â†’ BeautifulSoup â†’ RSS fallbacks
- âœ… **AI-Powered Content Cleaning**: Azure OpenAI GPT-4 removes boilerplate
- âœ… **Smart Duplicate Detection**: Semantic deduplication with title normalization
- âœ… **Multi-language Support**: Japanese content translation (Kabutan)

### **Production-Grade Storage**
- âœ… **Vector Database**: Qdrant with 3072-dimension embeddings for semantic search
- âœ… **Cloud Archive**: Azure Blob Storage with structured JSON documents
- âœ… **Intelligent Cleanup**: 24-hour document lifecycle management
- âœ… **Performance Caching**: Redis integration with in-memory fallback

### **Enterprise Monitoring & Alerting**
- âœ… **Azure Application Insights**: Cloud telemetry and performance tracking
- âœ… **Health Check API**: RESTful endpoints (`/health`, `/metrics`) 
- âœ… **Slack Integration**: Real-time failure notifications and alerts
- âœ… **Resource Monitoring**: Memory, CPU, and dependency health tracking
- âœ… **LLM Usage Analytics**: Token consumption tracking and cost management

### **Reliability & Performance**
- âœ… **Graceful Degradation**: Continues operation when optional services fail
- âœ… **Memory Management**: Automatic garbage collection and resource monitoring
- âœ… **Error Recovery**: Comprehensive retry mechanisms and fallback strategies
- âœ… **Concurrent Processing**: Async/await with rate limiting and timeout handling

## ğŸ“ˆ Supported Data Sources

| Source | Type | Content Focus | Rate Limit | Status |
|--------|------|---------------|------------|--------|
| **BabyPips** | RSS | Forex education & beginner analysis | 1s | âœ… Active |
| **FXStreet** | RSS | Professional forex market analysis | 1s | âœ… Active |
| **MarketWatch** | RSS | Market news and financial updates | 2s | âœ… Active |
| **Kabutan** | HTML | Japanese stock market analysis | 3s | âœ… Active |

*Additional sources easily configurable via `config/sources.yaml`*

## ğŸ—ï¸ Project Structure

```
newsraag-crawler/
â”œâ”€â”€ main.py                          # Enhanced main entry point
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sources.yaml                # Source configurations
â”œâ”€â”€ crawler/                        # Core crawling system
â”‚   â”œâ”€â”€ factories/                  # Source factory pattern
â”‚   â”œâ”€â”€ templates/                  # Processing templates (RSS, HTML)
â”‚   â”œâ”€â”€ extractors/                 # Content extraction engines
â”‚   â”œâ”€â”€ utils/                     # Crawler utilities and helpers
â”‚   â””â”€â”€ validators/                # Data validation and cleanup
â”œâ”€â”€ monitoring/                     # Comprehensive monitoring system
â”‚   â”œâ”€â”€ metrics.py                 # Local metrics collection
â”‚   â”œâ”€â”€ app_insights.py           # Azure telemetry integration
â”‚   â”œâ”€â”€ health_check.py           # Health monitoring APIs
â”‚   â”œâ”€â”€ alerts.py                 # Slack notification system
â”‚   â””â”€â”€ duplicate_detector.py     # Content deduplication
â”œâ”€â”€ clients/                       # External service integrations
â”‚   â”œâ”€â”€ qdrant_client.py          # Vector database operations
â”‚   â””â”€â”€ vector_client.py          # Embedding operations
â”œâ”€â”€ utils/                         # General utilities
â”‚   â”œâ”€â”€ llm/                      # AI content processing
â”‚   â””â”€â”€ config/                   # Configuration management
â”œâ”€â”€ models/                        # Data models and schemas
â”œâ”€â”€ tests/                        # Comprehensive test suite
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â””â”€â”€ fixtures/                 # Test data and mocks
â””â”€â”€ docs/                         # Documentation
```

## ğŸ”„ Data Processing Pipeline

### **1. Article Discovery**
```python
# Multi-strategy article discovery
RSS Feed â†’ Web Scraping â†’ Content Extraction
    â†“           â†“              â†“
Duplicate   Link           Article
Detection   Validation     Metadata
```

### **2. Content Processing**
```python
# AI-powered content cleaning pipeline
Raw HTML â†’ GPT-4 Cleaning â†’ Financial Data Preservation
    â†“            â†“                    â†“
Navigation   Content            Structured
Removal      Sanitization       Document
```

### **3. Storage & Indexing**
```python
# Multi-tier storage strategy
Clean Content â†’ Vector Embedding â†’ Database Storage
     â†“               â†“                  â†“
Azure Blob      Qdrant Index      Search Ready
Storage         (Semantic)        Documents
```

## ğŸ“Š Monitoring Dashboard

### **Health Check Endpoints**
```bash
# System health status
curl http://localhost:8001/health

# Detailed performance metrics  
curl http://localhost:8001/metrics
```

### **Real-time Statistics**
```
ğŸ“Š ENHANCED CRAWL CYCLE SUMMARY
=====================================
ğŸ¯ Overall Results:
   ğŸ“ˆ Articles discovered: 85
   âœ… Articles processed: 72
   âŒ Articles failed: 8
   â­ï¸ Articles skipped: 5
   ğŸ¯ Success rate: 84.7%
   ğŸ“¡ Sources: 4/4 active

ğŸ“‹ Per-Source Performance:
   ğŸ“¡ babypips: 18/20 (90% success)
   ğŸ“¡ fxstreet: 22/25 (88% success) 
   ğŸ“¡ marketwatch: 15/18 (83% success)
   ğŸ“¡ kabutan: 17/22 (77% success)
```

## âš™ï¸ Configuration

### **Environment Variables**
Essential configuration in `.env`:

```bash
# Azure OpenAI Services
OPENAI_BASE_URL=https://your-endpoint.openai.azure.com/
OPENAI_API_KEY=your_api_key
AZURE_OPENAI_DEPLOYMENT=gpt-4-1106-preview
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
EMBEDDING_DIMENSION=3072

# Vector Database
QDRANT_URL=https://your-qdrant-cluster.com
QDRANT_API_KEY=your_qdrant_api_key

# Azure Storage
AZ_ACCOUNT_NAME=your_storage_account
AZ_ACCOUNT_KEY=your_storage_key

# Monitoring (Optional)
APPINSIGHTS_INSTRUMENTATIONKEY=your_insights_key
ALERT_SLACK_WEBHOOK=your_slack_webhook_url
ALERT_SLACK_ENABLED=true

# Performance Settings
LLM_TOKEN_LIMIT_PER_REQUEST=4000
LLM_DAILY_TOKEN_LIMIT=1000000
```

### **Source Configuration**
Add new sources in `config/sources.yaml`:

```yaml
sources:
  - name: your_source
    type: rss  # or html_scraping
    url: https://example.com/feed.xml
    rate_limit: 2
    max_articles: 50
    content_type: forex
    # Optional HTML selectors
    selectors:
      title: ".article-title"
      content: ".article-body"
      link: "a.article-link"
```

## ğŸ§ª Testing

### **Run Tests**
```bash
# All tests
python run_tests.py all

# Unit tests only
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# Test with coverage
pytest --cov=crawler --cov=monitoring --cov-report=html

# Test specific functionality
pytest tests/unit/test_source_routing.py -v
```

### **Test Source Configuration**
```bash
# Verify source creation
python main.py --test-sources

# List available source types
python main.py --list-sources

# Check RSS feed health
python diagnose_rss_feeds.py
```

## ğŸš€ Deployment

### **Local Development**
```bash
# Standard development run
python main.py

# Clear vector database (fresh start)
python main.py --clear-collection

# Recreate database schema
python main.py --recreate-collection
```

### **Production Deployment**
```bash
# Use startup script for process monitoring
chmod +x startup.sh
./startup.sh

# Or with systemd service
sudo systemctl start newsraag-crawler
sudo systemctl enable newsraag-crawler
```

### **Azure App Service**
```bash
# Environment detection
PORT=8000  # Automatically detected
WEBSITE_HOSTNAME=your-app.azurewebsites.net
```

### **Azure Container Apps Jobs (Recommended for Cost Savings)**
For periodic crawling (e.g., every 3 hours), use **Container Apps Jobs** to pay only for active execution time:

1. **Deploy as a Job**:
   - Trigger type: **Schedule**
   - Cron expression: `0 */3 * * *` (Every 3 hours)
   - Command override: `python main.py --single-cycle`

2. **Benefits**:
   - **Cost**: $0 when idle (sleeping).
   - **Reliability**: Fresh container for every cycle, preventing memory leaks.

## ğŸ”§ Development Tools

### **Command Line Options**
```bash
# Development and testing
python main.py --test-sources        # Test source creation
python main.py --list-sources        # List available sources
python main.py --clear-collection    # Clear vector database
python main.py --recreate-collection # Recreate database schema

# Migration and backup
python migrate_main.py               # Safely migrate to enhanced version
```

### **Diagnostic Tools**
```bash
# RSS feed health check
python diagnose_rss_feeds.py

# Memory and performance analysis
python test_fixes.py

# Health monitoring
python test_health_check.py

# LLM integration testing
python integration_example.py
```

## ğŸ“ˆ Performance Metrics

### **Resource Usage**
- **Memory Baseline**: ~115MB with automatic cleanup at 800MB+
- **Processing Speed**: ~1.4 seconds per article (including AI cleaning)
- **LLM Efficiency**: ~2,100 tokens per article cleanup
- **Storage Compression**: 50% size reduction through intelligent cleaning
- **Uptime**: Continuous operation with automatic error recovery

### **Throughput Capacity**
- **Articles per Hour**: 50-200 depending on source configuration
- **Daily Processing**: 1,200-4,800 articles with default settings
- **Token Usage**: ~2.5M tokens per day (with 1,200 articles)
- **Storage Growth**: ~500MB-2GB per month of archived content

## ğŸ› ï¸ Troubleshooting

### **Common Issues**

**Issue**: Sources showing 0 articles discovered
```bash
# Check RSS feed health
python diagnose_rss_feeds.py

# Verify source configuration
python main.py --test-sources

# Check individual source logs
grep "Processing source" crawler.log
```

**Issue**: Memory usage warnings
```bash
# Normal for large batches - automatic cleanup
# Check memory monitoring logs:
grep "Memory usage" crawler.log

# Force garbage collection:
python -c "import gc; gc.collect()"
```

**Issue**: Vector database connection errors
```bash
# Check Qdrant connectivity
python -c "from clients.qdrant_client import get_qdrant_client; print('OK' if get_qdrant_client() else 'Failed')"

# Recreate collection if needed
python main.py --recreate-collection
```

### **Log Locations**
- **Application Logs**: Console output with structured JSON logging
- **Health Status**: Available via `http://localhost:8001/health`
- **Metrics Data**: `/data/metrics/` JSON files
- **Heartbeat**: `/data/heartbeat/crawler_heartbeat.txt`

### **Debug Mode**
```bash
# Enable verbose logging
export PYTHONPATH=$PYTHONPATH:$(pwd)
python main.py --verbose

# Test single cycle
python main.py --single-cycle  # (if implemented)
```

## ğŸ” Security Features

- **Environment-based Configuration**: No hardcoded credentials
- **Content Sanitization**: AI-powered content cleaning removes malicious content
- **URL Validation**: Comprehensive URL validation and sanitization  
- **Rate Limiting**: Respectful crawling with configurable delays
- **Memory Limits**: Automatic resource monitoring and cleanup
- **Path Validation**: Secure file operations with path validation

## ğŸ“š Advanced Features

### **AI Content Processing**
```python
# GPT-4 powered content cleaning
Content â†’ Remove Navigation â†’ Preserve Financial Data â†’ Clean Output
         Remove Ads           Extract Key Info      Structure JSON
```

### **Vector Similarity Search**
```python
# Semantic article search and deduplication
Query â†’ Embedding â†’ Vector Search â†’ Ranked Results
       (3072-dim)   (Qdrant)       (Similarity)
```

### **Intelligent Monitoring**
```python
# Multi-layer health monitoring
Application â†’ Resource â†’ Dependencies â†’ Cloud
Health        Usage      (Qdrant/Azure)  (Insights)
```

## ğŸ¤ Contributing

### **Development Setup**
```bash
# Fork and clone repository
git clone https://github.com/yourusername/newsraag-crawler.git

# Install development dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # If available

# Run tests before changes
python run_tests.py all

# Follow coding standards
black . && flake8 .
```

### **Adding New Sources**
1. Add source configuration to `config/sources.yaml`
2. Test with `python main.py --test-sources`
3. Create unit tests for new source types
4. Update documentation

### **Architecture Principles**
- **Factory Pattern**: Use `SourceFactory` for new source types
- **Template Method**: Extend existing templates for consistency
- **Graceful Degradation**: Always provide fallbacks for failures
- **Comprehensive Logging**: Log all operations with structured data

## ğŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Azure OpenAI**: AI-powered content processing
- **Qdrant**: High-performance vector database
- **Crawl4AI**: Modern web scraping capabilities
- **Playwright**: Reliable browser automation
- **FastAPI**: Health check API framework

---

**Built for production financial news processing with enterprise-grade reliability, monitoring, and AI-powered quality enhancement.**
